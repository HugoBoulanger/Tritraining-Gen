

BERT:
  dataset:
    # Special slot label value specifying that the token
    # should be ignored during loss and performance calculation
    ignore_index: -100
    # Whether to remove utterances whose length doesn't match slot labels length
    remove_bad_alignments: true
    # Whether to lowercase utterances
    do_lowercase: false
    # Whether to label all subwords or only the first one and ignore the others
    label_all_subwords: false

  model:
    # Model name (huggingface) or path to model checkpoint directory to load the model and tokenizer.
    # If it's a name, it will download the checkpoint from huggingface.co
    name_or_path:

    # Path to seqeval.py script to load the metric locally (offline mode).
    # Download the original script: https://github.com/huggingface/datasets/blob/master/metrics/seqeval/seqeval.py
    # Remove the value to load it from huggingface.co when used.
    seqeval_path:

  train:
    # Fix seed for reproducibility
    seed: 1
    # Batch size to use during training
    batch_size: 16
    # Validation metric to look at for model selection: intent_acc|slot_f1|loss. Defaults to 'slot_f1'
    validation_metric: 'slot_f1'
    # Whether train for intent detection as well as slot filling. Defaults to False
    do_intent_detection: false
    # Whether to freeze BERT weights. Defaults to False
    freeze_bert: false
    # The dropout probability to use on output embeddings before classification. float. Defaults to 0.1.
    dropout: 0.1
    # Learning rate to train the model. float. Defaults to 1e-5.
    learning_rate: 1e-5
    # The weight to give to the slot filling loss during training. float. Defaults to 1.
    slot_loss_coeff: 1
    # The number of epochs to train each language. Defaults to 20.
    epochs_per_lang: 100
    # The number of workers to use for data fetching. Optional, defaults to half the number of processors.
    num_workers: 4
    # Whether to keep checkpoints in disk. Can be deactivated to save disk space. Defaults to True.
    keep_checkpoints: true
    # The languages to train the model on.
    languages: [ 'EN' ]
    # Whether to keep training the model or not
    keep_training: true


tri-train:
  preprocess:
    # Path to TSV files for train, dev and test
    path:
    # Whether this operation was completed by tri training, default is false
    done: false
    # Seed for the randomization of training data
    seed: 1
    # Max size of the training data
    size: 1000
    # Split: size of the split of training data actually used in this particular experiment (the inclusion spilts)
    split: 50
    # Generator path
    gen_path:
    # GPT 2 path for generation at every step
    gpt2_path:
    # T5 path for generation at every step
    t5_path:
    # Method of generation
    method:
    # How many sentence to generate per initial sentences during generation process
    num_generated: 5

  tritraining:
    # Whether to work with the natural unlabeled set.
    append_unlabeled: false
    # Whether the preparation of data (copy of files, etc...) is done
    data_prep_done : false
    # Whether tritraining has finished training (used to continue experiments that were stopped prematurely)
    tritraining_done : false
    # Whether training the baseline is done (can be used to avoid training a baseline)
    baseline_done : false
    # If static generation, whether it has been
    generation_done : false
    # Whether pretraining has been done.
    pretrain_done : false
    # Whether to generate at each episode
    generate_episodic: false
    # full_pretrain : false for pretraining on sampled set, true for a round of pretraining on sampled + a round of pretraining on full dataset
    full_pretrain: false
    # number of the current episode
    current_episode: 0
    # number of the current tritraining model to train
    current_tri: 1
    # Number of episodes
    episodes: 20
    self_training: false

  model:
    # Model name (huggingface) or path to model checkpoint directory to load the model and tokenizer.
    # If it's a name, it will download the checkpoint from huggingface.co
    name_or_path:
    # Path to the seqeval path
    seqeval_path:

  dataset:
    # Path to TSV files for train, dev and test after preprocessing
    path:
    # Special slot label value specifying that the token
    # should be ignored during loss and performance calculation
    ignore_index: -100

  train:
    batch_size: 16
    num_workers: 4